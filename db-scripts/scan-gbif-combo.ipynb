{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import re\n",
    "import pymysql\n",
    "import configparser as cp\n",
    "import pykml.parser\n",
    "\n",
    "from omoccurrences_dtypes import OMOCCURRENCES_DTYPES\n",
    "from datetime import datetime\n",
    "from pymysql.cursors import DictCursor\n",
    "from sys import stderr\n",
    "from os import environ, path, remove as os_remove\n",
    "from IPython.display import display\n",
    "from shutil import copy2 as shutil_copy\n",
    "from zipfile import ZipFile, ZIP_DEFLATED\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "from lib import get_mysql_conn, get_sqlite_conn\n",
    "\n",
    "pandarallel.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNTRIES_FILE = \"countries.txt\"\n",
    "GBIF_CSV = \"/data/disk/jupyter-notebooks/GBIF_bee_occurrences_2021_harvest.csv\"\n",
    "N_AMERICA_KML = \"NAmerica.kml\"\n",
    "PROVINCES_FILE = \"provinces.txt\"\n",
    "SCAN_SQLITE = \"/data/disk/jupyter-notebooks/{}_symbscan.sqlite\".format(datetime.now().strftime(\"%F\"))\n",
    "COMBINED_SQLITE = \"/data/disk/jupyter-notebooks/{}_scan_gbif.sqlite\".format(datetime.now().strftime(\"%F\"))\n",
    "SQL_CONFIG_FILE = path.join(environ[\"HOME\"], \".my.cnf\")\n",
    "\n",
    "\n",
    "def get_kml_poly(kml_file_name):\n",
    "    with open(kml_file_name, \"rb\") as f:\n",
    "        kml_file = pykml.parser.fromstring(f.read())\n",
    "\n",
    "    kml_coords = str(kml_file.Document.Placemark.Polygon.outerBoundaryIs.LinearRing.coordinates).strip()\n",
    "    kml_coords = [p for p in kml_coords.split(\" \")]\n",
    "    kml_coords = [(float(lng), float(lat)) for lng, lat, alt in [p.split(\",\") for p in kml_coords]]\n",
    "    return kml_coords\n",
    "\n",
    "\n",
    "def get_provinces(file):\n",
    "    with open(file) as f:\n",
    "        return [l.strip() for l in f.readlines() if l != \"\"]\n",
    "\n",
    "    \n",
    "def get_countries(file):\n",
    "    with open(file) as f:\n",
    "        return [l.strip() for l in f.readlines() if l != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "KML_POLY = get_kml_poly(N_AMERICA_KML)\n",
    "OMOCCURRENCES_LATS = [p[0] for p in KML_POLY]\n",
    "OMOCCURRENCES_LNGS = [p[1] for p in KML_POLY]\n",
    "\n",
    "OMOCCURRENCES_LATITUDE_RANGE = [min(OMOCCURRENCES_LATS), max(OMOCCURRENCES_LATS)]\n",
    "OMOCCURRENCES_LONGITUDE_RANGE = [min(OMOCCURRENCES_LNGS), max(OMOCCURRENCES_LNGS)]\n",
    "\n",
    "TARGET_FAMILIES = [\n",
    "    'andrenidae',\n",
    "    'apidae',\n",
    "    'colletidae',\n",
    "    'halictidae',\n",
    "    'megachilidae',\n",
    "    'melittidae',\n",
    "]\n",
    "\n",
    "TARGET_COLUMNS = sorted(OMOCCURRENCES_DTYPES.keys())\n",
    "TARGET_COUNTRIES = get_countries(COUNTRIES_FILE)\n",
    "TARGET_PROVINCES = get_provinces(PROVINCES_FILE)\n",
    "\n",
    "SCAN_QUERY = \"\"\"\n",
    "    SELECT \n",
    "        {}, \n",
    "        c.collectionCode as collectionCode, \n",
    "        c.institutionCode as institutionCode \n",
    "        \n",
    "    FROM omoccurrences o\n",
    "    \n",
    "    INNER JOIN omcollections c\n",
    "    ON o.collid = c.collid\n",
    "    \n",
    "    WHERE (\n",
    "        sciName IS NOT NULL\n",
    "        AND sciName != ''\n",
    "        AND LOWER(family) IN ({})\n",
    "        AND (\n",
    "            (\n",
    "                o.decimalLatitude BETWEEN {} AND {} \n",
    "                AND o.decimalLongitude BETWEEN {} AND {}\n",
    "            )\n",
    "            OR lower(o.country) in (\n",
    "                {}\n",
    "            )\n",
    "            OR lower(o.stateProvince) in (\n",
    "                {}\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\"\"\".format(\n",
    "    ',\\n\\t'.join([\"o.{0} as {0}\".format(c) for c in TARGET_COLUMNS]),\n",
    "    ','.join([\"'{}'\".format(f) for f in TARGET_FAMILIES]),\n",
    "    *OMOCCURRENCES_LATITUDE_RANGE,\n",
    "    *OMOCCURRENCES_LONGITUDE_RANGE,\n",
    "    ',\\n\\t\\t'.join([\"'{}'\".format(f) for f in TARGET_COUNTRIES]),\n",
    "    ',\\n\\t\\t'.join([\"'{}'\".format(f) for f in TARGET_PROVINCES]),\n",
    ")\n",
    "\n",
    "\n",
    "def get_scan_query(limit, offset):\n",
    "    return \"{} LIMIT {} OFFSET {}\".format(SCAN_QUERY, limit, offset)\n",
    "\n",
    "#print(SCAN_QUERY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIMIT = 500000\n",
    "offset = 0\n",
    "\n",
    "if path.exists(SCAN_SQLITE):\n",
    "    os_remove(SCAN_SQLITE)\n",
    "\n",
    "def populate_scientificName_scan(row):\n",
    "    row[\"scientificName\"] = row[\"sciname\"]\n",
    "    return row\n",
    "\n",
    "target_dtypes = OMOCCURRENCES_DTYPES.copy()\n",
    "target_dtypes.update({\n",
    "    \"collectionCode\": np.dtype(\"unicode\"),\n",
    "    \"institutionCode\": np.dtype(\"unicode\"),\n",
    "    \"source\": np.dtype(\"unicode\")\n",
    "})\n",
    "\n",
    "scan_conn = get_mysql_conn(SQL_CONFIG_FILE)\n",
    "sqlite_conn = get_sqlite_conn(SCAN_SQLITE)\n",
    "\n",
    "try:\n",
    "    with scan_conn:\n",
    "        with scan_conn.cursor() as cursor:\n",
    "            row_count = cursor.execute(get_scan_query(LIMIT, offset))   \n",
    "            \n",
    "            while row_count > 0:\n",
    "                chunk = cursor.fetchall()\n",
    "                chunk_df = pd.DataFrame(chunk)\n",
    "                chunk_df['source'] = 'scan'\n",
    "                \n",
    "                chunk_df = chunk_df[target_dtypes.keys()].astype(target_dtypes)\n",
    "                chunk_df = chunk_df.parallel_apply(populate_scientificName_scan, axis=\"columns\")\n",
    "\n",
    "                with sqlite_conn:\n",
    "                    chunk_df.to_sql(\n",
    "                        \"omoccurrences\",\n",
    "                        con=sqlite_conn,\n",
    "                        index=False,\n",
    "                        if_exists=\"append\"\n",
    "                    )\n",
    "                \n",
    "                offset += row_count\n",
    "                row_count = cursor.execute(get_scan_query(LIMIT, offset))\n",
    "                \n",
    "except Exception as e:\n",
    "    try:\n",
    "        scan_conn.close()\n",
    "    \n",
    "        sqlite_conn.rollback()\n",
    "        sqlite_conn.close()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    raise e\n",
    "    \n",
    "sqlite_conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbif_df = pd.read_csv(GBIF_CSV, sep=\"\\t\", nrows=1)\n",
    "gbif_cols = sorted(list(gbif_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil_copy(SCAN_SQLITE, COMBINED_SQLITE)\n",
    "\n",
    "sqlite_conn = get_sqlite_conn(COMBINED_SQLITE)\n",
    "\n",
    "try:\n",
    "    with sqlite_conn:\n",
    "        scan_cols_query = sqlite_conn.execute(\"select * from omoccurrences limit 1\")\n",
    "        scan_cols = sorted([desc[0] for desc in scan_cols_query.description])\n",
    "                \n",
    "except Exception as e:\n",
    "    print(e, file=stderr)\n",
    "    \n",
    "finally:\n",
    "    sqlite_conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "common_cols = list(np.intersect1d(gbif_cols, scan_cols))\n",
    "#[print(c) for c in common_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 500000\n",
    "\n",
    "def populate_scientificName_gbif(row):\n",
    "    rank_col = row[\"taxonRank\"].lower()\n",
    "    if rank_col in row:\n",
    "        row[\"scientificName\"] = row[rank_col]\n",
    "    return row\n",
    "\n",
    "sqlite_conn = get_sqlite_conn(COMBINED_SQLITE)\n",
    "\n",
    "try:   \n",
    "    gbif_df = pd.read_csv(GBIF_CSV, chunksize=CHUNK_SIZE, sep=\"\\t\", low_memory=False)\n",
    "    for chunk in gbif_df:\n",
    "        chunk['source'] = 'gbif'\n",
    "        chunk = chunk.parallel_apply(populate_scientificName_gbif, axis='columns')\n",
    "        chunk = chunk[['source', *common_cols]]\n",
    "        #display(chunk[\"scientificName\"].head())\n",
    "        #break\n",
    "\n",
    "        with sqlite_conn:\n",
    "            chunk.to_sql(\n",
    "                \"omoccurrences\",\n",
    "                con=sqlite_conn,\n",
    "                index=False,\n",
    "                if_exists=\"append\"\n",
    "            )\n",
    "            \n",
    "except Exception as e:\n",
    "    sqlite_conn.rollback()\n",
    "    sqlite_conn.close()\n",
    "    raise e\n",
    "\n",
    "sqlite_conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ZIP_FILE = \"{}.zip\".format(COMBINED_SQLITE)\n",
    "\n",
    "if path.exists(ZIP_FILE):\n",
    "    os_remove(ZIP_FILE)\n",
    "\n",
    "with ZipFile(ZIP_FILE, 'w', compression=ZIP_DEFLATED) as zipfile:\n",
    "    zipfile.write(COMBINED_SQLITE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
