{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import re\n",
    "import pymysql\n",
    "import configparser as cp\n",
    "import pykml.parser\n",
    "\n",
    "from omoccurrences_dtypes import OMOCCURRENCES_DTYPES\n",
    "from datetime import datetime\n",
    "from pymysql.cursors import DictCursor\n",
    "from sys import stderr\n",
    "from os import environ, path, remove as os_remove\n",
    "from IPython.display import display\n",
    "from gc import collect as gc_collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNTRIES_FILE = \"countries.txt\"\n",
    "GBIF_CSV = \"/data/disk/jupyter-notebooks/GBIF_bee_occurrences_2021_harvest.csv\"\n",
    "N_AMERICA_KML = \"NAmerica.kml\"\n",
    "PROVINCES_FILE = \"provinces.txt\"\n",
    "SCAN_SQLITE = \"/data/disk/jupyter-notebooks/{}_symbscan.sqlite\".format(datetime.now().strftime(\"%F\"))\n",
    "SQL_CONFIG_FILE = path.join(environ[\"HOME\"], \".my.cnf\")\n",
    "\n",
    "\n",
    "def get_mysql_config(file):\n",
    "    config = cp.ConfigParser()\n",
    "    config.read(file)\n",
    "    return {\n",
    "        \"host\": config[\"client\"][\"host\"],\n",
    "        \"port\": int(config[\"client\"][\"port\"]),\n",
    "        \"user\": config[\"client\"][\"user\"],\n",
    "        \"password\": config[\"client\"][\"password\"],\n",
    "        \"database\": config[\"mysql\"][\"database\"],\n",
    "        \"charset\": \"utf8mb4\",\n",
    "        \"cursorclass\": DictCursor,\n",
    "    }\n",
    "\n",
    "\n",
    "def get_sqlite_conn(file):\n",
    "    sqlite_conn = sqlite3.connect(file)\n",
    "    sqlite_conn.row_factory = sqlite3.Row\n",
    "    return sqlite_conn\n",
    "\n",
    "\n",
    "def get_mysql_conn(config_file):\n",
    "    config = get_mysql_config(config_file)\n",
    "    return pymysql.connect(**config)\n",
    "\n",
    "\n",
    "def get_kml_poly(kml_file_name):\n",
    "    with open(kml_file_name, \"rb\") as f:\n",
    "        kml_file = pykml.parser.fromstring(f.read())\n",
    "\n",
    "    kml_coords = str(kml_file.Document.Placemark.Polygon.outerBoundaryIs.LinearRing.coordinates).strip()\n",
    "    kml_coords = [p for p in kml_coords.split(\" \")]\n",
    "    kml_coords = [(float(lng), float(lat)) for lng, lat, alt in [p.split(\",\") for p in kml_coords]]\n",
    "    return kml_coords\n",
    "\n",
    "\n",
    "def get_provinces(file):\n",
    "    with open(file) as f:\n",
    "        return [l.strip() for l in f.readlines() if l != \"\"]\n",
    "\n",
    "    \n",
    "def get_countries(file):\n",
    "    with open(file) as f:\n",
    "        return [l.strip() for l in f.readlines() if l != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "KML_POLY = get_kml_poly(N_AMERICA_KML)\n",
    "OMOCCURRENCES_LATS = [p[0] for p in KML_POLY]\n",
    "OMOCCURRENCES_LNGS = [p[1] for p in KML_POLY]\n",
    "\n",
    "OMOCCURRENCES_LATITUDE_RANGE = [min(OMOCCURRENCES_LATS), max(OMOCCURRENCES_LATS)]\n",
    "OMOCCURRENCES_LONGITUDE_RANGE = [min(OMOCCURRENCES_LNGS), max(OMOCCURRENCES_LNGS)]\n",
    "\n",
    "TARGET_FAMILIES = [\n",
    "    'melittidae',\n",
    "    'colletidae',\n",
    "    'apidae',\n",
    "    'megachilidae',\n",
    "    'halictidae',\n",
    "    'andrenidae'\n",
    "]\n",
    "\n",
    "TARGET_COUNTRIES = get_countries(COUNTRIES_FILE)\n",
    "TARGET_PROVINCES = get_provinces(PROVINCES_FILE)\n",
    "\n",
    "SCAN_QUERY = \"\"\"\n",
    "    SELECT * FROM omoccurrences\n",
    "    WHERE (\n",
    "        LOWER(family) IN ({})\n",
    "        AND (\n",
    "            (\n",
    "                decimalLatitude BETWEEN {} AND {} \n",
    "                AND decimalLongitude BETWEEN {} AND {}\n",
    "            )\n",
    "            OR lower(country) in ({})\n",
    "            OR lower(stateProvince) in ({})\n",
    "        )\n",
    "    )\n",
    "\"\"\".format(\n",
    "    ','.join([\"'{}'\".format(f) for f in TARGET_FAMILIES]),\n",
    "    *OMOCCURRENCES_LATITUDE_RANGE,\n",
    "    *OMOCCURRENCES_LONGITUDE_RANGE,\n",
    "    ','.join([\"'{}'\".format(f) for f in TARGET_COUNTRIES]),\n",
    "    ','.join([\"'{}'\".format(f) for f in TARGET_PROVINCES]),\n",
    ")\n",
    "\n",
    "\n",
    "def get_scan_query(limit, offset):\n",
    "    return \"{} LIMIT {} OFFSET {}\".format(SCAN_QUERY, limit, offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIMIT = 100000\n",
    "offset = 0\n",
    "\n",
    "def populate_scientificName(row):\n",
    "    row[\"scientificName\"] = row[\"sciname\"]\n",
    "    return row\n",
    "\n",
    "scan_conn = get_mysql_conn(SQL_CONFIG_FILE)\n",
    "\n",
    "try:\n",
    "    with scan_conn:\n",
    "        with scan_conn.cursor() as cursor:\n",
    "            row_count = cursor.execute(get_scan_query(LIMIT, offset))   \n",
    "            input_df = pd.DataFrame()\n",
    "            \n",
    "            while row_count > 0:\n",
    "                chunk = cursor.fetchall()\n",
    "                input_df = pd.concat([input_df, pd.DataFrame(chunk)], ignore_index=True)\n",
    "                offset += row_count\n",
    "                row_count = cursor.execute(get_scan_query(LIMIT, offset))\n",
    "                \n",
    "except Exception as e:\n",
    "    scan_conn.rollback()\n",
    "    scan_conn.close()\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan_df = input_df.copy().astype(OMOCCURRENCES_DTYPES)\n",
    "\n",
    "scan_df['source'] = 'scan'\n",
    "scan_df = scan_df.apply(populate_scientificName, axis=\"columns\")\n",
    "\n",
    "display(mod_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if path.exists(SCAN_SQLITE):\n",
    "    os_remove(SCAN_SQLITE)\n",
    "\n",
    "sqlite_conn = get_sqlite_conn(SCAN_SQLITE)\n",
    "\n",
    "try:\n",
    "    with sqlite_conn:\n",
    "        mod_df.to_sql(\n",
    "            \"omoccurrences\",\n",
    "            con=sqlite_conn,\n",
    "            index=False\n",
    "        )\n",
    "    \n",
    "except Exception as e:\n",
    "    sqlite_conn.rollback()\n",
    "    sqlite_conn.close()\n",
    "    raise e\n",
    "\n",
    "sqlite_conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbif_df = pd.read_csv(GBIF_CSV, sep=\"\\t\", nrows=1)\n",
    "gbif_cols = sorted(list(gbif_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlite_conn = get_sqlite_conn(SCAN_SQLITE)\n",
    "\n",
    "try:\n",
    "    with sqlite_conn:\n",
    "        scan_cols_query = sqlite_conn.execute(\"select * from omoccurrences limit 1\")\n",
    "        scan_cols = sorted([desc[0] for desc in scan_cols_query.description])\n",
    "                \n",
    "except Exception as e:\n",
    "    print(e, file=stderr)\n",
    "    \n",
    "finally:\n",
    "    sqlite_conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "common_cols = list(np.intersect1d(gbif_cols, scan_cols))\n",
    "#[print(c) for c in common_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 100000\n",
    "\n",
    "def populate_scientificName(row):\n",
    "    rank_col = row[\"taxonRank\"].lower()\n",
    "    if rank_col in row:\n",
    "        row[\"scientificName\"] = row[rank_col]\n",
    "    return row\n",
    "\n",
    "sqlite_conn = get_sqlite_conn(SCAN_SQLITE)\n",
    "\n",
    "try:   \n",
    "    with sqlite_conn:\n",
    "        gbif_df = pd.read_csv(GBIF_CSV, chunksize=CHUNK_SIZE, sep=\"\\t\", low_memory=False)\n",
    "        for chunk in gbif_df:\n",
    "            chunk['source'] = 'gbif'\n",
    "            chunk = chunk.apply(populate_sciname, axis='columns')\n",
    "            chunk = chunk[['source', *common_cols]]\n",
    "            #display(chunk[\"scientificName\"].head())\n",
    "            #break\n",
    "            \n",
    "            chunk.to_sql(\n",
    "                \"omoccurrences\",\n",
    "                con=sqlite_conn,\n",
    "                index=False,\n",
    "                if_exists=\"append\"\n",
    "            )\n",
    "            \n",
    "except Exception as e:\n",
    "    print(e, file=stderr)\n",
    "    sqlite_conn.rollback()\n",
    "\n",
    "finally:\n",
    "    sqlite_conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlite_conn = get_sqlite_conn(SCAN_SQLITE)\n",
    "\n",
    "try:\n",
    "    with sqlite_conn:\n",
    "        # GBIF stores it as scientificName, SCAN stores it as sciName\n",
    "        sqlite_conn.execute(\"update omoccurrences set scientificName = sciName where source = 'scan'\")\n",
    "        \n",
    "        # Cleaning\n",
    "        sqlite_conn.execute(\"update omoccurrences set scientificName = trim(scientificName)\")\n",
    "        sqlite_conn.execute(\"update omoccurrences set decimalLatitude = round(decimalLatitude, 4)\")\n",
    "        sqlite_conn.execute(\"update omoccurrences set decimalLongitude = round(decimalLongitude, 4)\")\n",
    "except Exception as e:\n",
    "    print(e, file=stderr)\n",
    "    sqlite_conn.rollback()\n",
    "finally:\n",
    "    sqlite_conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_parenthesis(row):\n",
    "    sciName = row[\"scientificName\"]\n",
    "    \n",
    "    if pd.isna(sciName):\n",
    "        return row\n",
    "    \n",
    "    sciName = re.sub(r\"\\([^)]+\\)\", \"\", sciName)\n",
    "    sciName = re.sub(r\" +\", \" \", sciName).strip()\n",
    "    row[\"scientificName\"] = sciName\n",
    "    return row\n",
    "\n",
    "def remove_authorship(row):\n",
    "    sciName = row[\"scientificName\"]\n",
    "    \n",
    "    if pd.isna(sciName):\n",
    "        return row\n",
    "    \n",
    "    sciName = re.sub(r\" [A-Z][a-z]+,\\s+\\d{4}\\s*$\", \"\", sciName)\n",
    "    sciName = re.sub(r\" +\", \" \", sciName).strip()\n",
    "    row[\"scientificName\"] = sciName\n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'remove_parenthesis' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-0c8ec658a98c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0msqlite_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0msqlite_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0msqlite_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-0c8ec658a98c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#             display(chunk.head(n=N))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremove_parenthesis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m#             print(\"Without parentheses\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'remove_parenthesis' is not defined"
     ]
    }
   ],
   "source": [
    "# ===== FILTERING =====\n",
    "\n",
    "CHUNK_SIZE = 100000\n",
    "N=10\n",
    "\n",
    "sqlite_conn = get_sqlite_conn(SCAN_SQLITE)\n",
    "    \n",
    "try:\n",
    "    with sqlite_conn:\n",
    "        db_df = pd.read_sql(\"select occid, scientificName from omoccurrences\", sqlite_conn, chunksize=CHUNK_SIZE)\n",
    "        for chunk in db_df:\n",
    "#             print(\"Original\")\n",
    "#             display(chunk.head(n=N))\n",
    "            \n",
    "            chunk = chunk.apply(remove_parenthesis, axis=\"columns\")\n",
    "            \n",
    "#             print(\"Without parentheses\")\n",
    "#             display(chunk.head(n=N))\n",
    "            \n",
    "            chunk = chunk.apply(remove_authorship, axis=\"columns\")\n",
    "            \n",
    "#             print(\"Without authorship\")\n",
    "#             display(chunk.head(n=N))\n",
    "            \n",
    "            chunk_valid_scinames = chunk[\n",
    "                ~(pd.isna(chunk[\"scientificName\"]) | (chunk[\"scientificName\"] == ''))\n",
    "            ]\n",
    "            for _, row in chunk_valid_scinames.iterrows():\n",
    "                sqlite_conn.execute(\n",
    "                    \"UPDATE omoccurrences SET scientificName = ? WHERE occid = ?\",\n",
    "                    (row[\"scientificName\"], row[\"occid\"])\n",
    "                )\n",
    "\n",
    "except Exception as e:\n",
    "    sqlite_conn.rollback()\n",
    "    sqlite_conn.close()\n",
    "    raise e\n",
    "\n",
    "sqlite_conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
