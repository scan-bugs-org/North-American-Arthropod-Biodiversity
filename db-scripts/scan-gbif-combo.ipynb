{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import re\n",
    "import pymysql\n",
    "import configparser as cp\n",
    "import pykml.parser\n",
    "\n",
    "from datetime import datetime\n",
    "from pymysql.cursors import DictCursor\n",
    "from sys import stderr\n",
    "from os import environ, path, remove as os_remove\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNTRIES_FILE = \"countries.txt\"\n",
    "GBIF_CSV = \"GBIF_bee_occurrences_2021_harvest.csv\"\n",
    "N_AMERICA_KML = \"NAmerica.kml\"\n",
    "PROVINCES_FILE = \"provinces.txt\"\n",
    "SCAN_SQLITE = \"{}_symbscan.sqlite\".format(datetime.now().strftime(\"%F\"))\n",
    "SQL_CONFIG_FILE = path.join(environ[\"HOME\"], \".my.cnf\")\n",
    "\n",
    "\n",
    "def get_mysql_config(file):\n",
    "    config = cp.ConfigParser()\n",
    "    config.read(file)\n",
    "    return {\n",
    "        \"host\": config[\"client\"][\"host\"],\n",
    "        \"port\": int(config[\"client\"][\"port\"]),\n",
    "        \"user\": config[\"client\"][\"user\"],\n",
    "        \"password\": config[\"client\"][\"password\"],\n",
    "        \"database\": config[\"mysql\"][\"database\"],\n",
    "        \"charset\": \"utf8mb4\",\n",
    "        \"cursorclass\": DictCursor,\n",
    "    }\n",
    "\n",
    "\n",
    "def get_sqlite_conn(file):\n",
    "    sqlite_conn = sqlite3.connect(file)\n",
    "    sqlite_conn.row_factory = sqlite3.Row\n",
    "    return sqlite_conn\n",
    "\n",
    "\n",
    "def get_mysql_conn(config_file):\n",
    "    config = get_mysql_config(config_file)\n",
    "    return pymysql.connect(**config)\n",
    "\n",
    "\n",
    "def get_kml_poly(kml_file_name):\n",
    "    with open(kml_file_name, \"rb\") as f:\n",
    "        kml_file = pykml.parser.fromstring(f.read())\n",
    "\n",
    "    kml_coords = str(kml_file.Document.Placemark.Polygon.outerBoundaryIs.LinearRing.coordinates).strip()\n",
    "    kml_coords = [p for p in kml_coords.split(\" \")]\n",
    "    kml_coords = [(float(lng), float(lat)) for lng, lat, alt in [p.split(\",\") for p in kml_coords]]\n",
    "    return kml_coords\n",
    "\n",
    "\n",
    "def get_provinces(file):\n",
    "    with open(file) as f:\n",
    "        return [l.strip() for l in f.readlines() if l != \"\"]\n",
    "\n",
    "    \n",
    "def get_countries(file):\n",
    "    with open(file) as f:\n",
    "        return [l.strip() for l in f.readlines() if l != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KML_POLY = get_kml_poly(N_AMERICA_KML)\n",
    "OMOCCURRENCES_LATS = [p[0] for p in KML_POLY]\n",
    "OMOCCURRENCES_LNGS = [p[1] for p in KML_POLY]\n",
    "\n",
    "OMOCCURRENCES_LATITUDE_RANGE = [min(OMOCCURRENCES_LATS), max(OMOCCURRENCES_LATS)]\n",
    "OMOCCURRENCES_LONGITUDE_RANGE = [min(OMOCCURRENCES_LNGS), max(OMOCCURRENCES_LNGS)]\n",
    "\n",
    "TARGET_FAMILIES = [\n",
    "    'melittidae',\n",
    "    'colletidae',\n",
    "    'apidae',\n",
    "    'megachilidae',\n",
    "    'halictidae',\n",
    "    'andrenidae'\n",
    "]\n",
    "\n",
    "TARGET_COUNTRIES = get_countries(COUNTRIES_FILE)\n",
    "TARGET_PROVINCES = get_provinces(PROVINCES_FILE)\n",
    "\n",
    "SCAN_QUERY = \"\"\"\n",
    "    SELECT * FROM omoccurrences\n",
    "    WHERE (\n",
    "        LOWER(family) IN ({})\n",
    "        AND (\n",
    "            (\n",
    "                decimalLatitude BETWEEN {} AND {} \n",
    "                AND decimalLongitude BETWEEN {} AND {}\n",
    "            )\n",
    "            OR lower(country) in ({})\n",
    "            OR lower(stateProvince) in ({})\n",
    "        )\n",
    "    )\n",
    "\"\"\".format(\n",
    "    ','.join([\"'{}'\".format(f) for f in TARGET_FAMILIES]),\n",
    "    *OMOCCURRENCES_LATITUDE_RANGE,\n",
    "    *OMOCCURRENCES_LONGITUDE_RANGE,\n",
    "    ','.join([\"'{}'\".format(f) for f in TARGET_COUNTRIES]),\n",
    "    ','.join([\"'{}'\".format(f) for f in TARGET_PROVINCES]),\n",
    ")\n",
    "\n",
    "#print(SCAN_QUERY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE=50000\n",
    "\n",
    "if path.exists(SCAN_SQLITE):\n",
    "    os_remove(SCAN_SQLITE)\n",
    "\n",
    "scan_conn = get_mysql_conn(SQL_CONFIG_FILE)\n",
    "sqlite_conn = get_sqlite_conn(SCAN_SQLITE)\n",
    "\n",
    "try:\n",
    "    with scan_conn:\n",
    "        input_df = pd.read_sql(SCAN_QUERY, scan_conn, chunksize=CHUNK_SIZE)\n",
    "        \n",
    "        for chunk in input_df:\n",
    "            chunk['source'] = 'scan'\n",
    "            with sqlite_conn:\n",
    "                chunk.to_sql(\n",
    "                    \"omoccurrences\",\n",
    "                    con=sqlite_conn,\n",
    "                    index=False,\n",
    "                    if_exists=\"append\"\n",
    "                )\n",
    "\n",
    "except Exception as e:\n",
    "    sqlite_conn.rollback()\n",
    "    sqlite_conn.close()\n",
    "    \n",
    "    scan_conn.rollback()\n",
    "    scan_conn.close()\n",
    "    raise e\n",
    "\n",
    "sqlite_conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reset_selective -f ^chunk$\n",
    "%reset_selective -f ^input_df$\n",
    "%reset_selective -f ^sqlite_conn$\n",
    "%reset_selective -f ^scan_conn$\n",
    "gc_collect()\n",
    "\n",
    "#%who"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbif_df = pd.read_csv(GBIF_CSV, sep=\"\\t\", nrows=1)\n",
    "gbif_cols = sorted(list(gbif_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlite_conn = get_sqlite_conn(SCAN_SQLITE)\n",
    "\n",
    "try:\n",
    "    with sqlite_conn:\n",
    "        scan_cols_query = sqlite_conn.execute(\"select * from omoccurrences limit 1\")\n",
    "        scan_cols = sorted([desc[0] for desc in scan_cols_query.description])\n",
    "                \n",
    "except Exception as e:\n",
    "    print(e, file=stderr)\n",
    "    \n",
    "finally:\n",
    "    sqlite_conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "common_cols = list(np.intersect1d(gbif_cols, scan_cols))\n",
    "#[print(c) for c in common_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 100000\n",
    "\n",
    "sqlite_conn = get_sqlite_conn(SCAN_SQLITE)\n",
    "\n",
    "try:   \n",
    "    with sqlite_conn:\n",
    "        gbif_df = pd.read_csv(GBIF_CSV, chunksize=CHUNK_SIZE, sep=\"\\t\", low_memory=False)\n",
    "        for chunk in gbif_df:\n",
    "            chunk['source'] = 'gbif'\n",
    "            chunk = chunk[['source', *common_cols]]\n",
    "            #display(chunk[\"scientificName\"].head())\n",
    "            #break\n",
    "            \n",
    "            chunk.to_sql(\n",
    "                \"omoccurrences\",\n",
    "                con=sqlite_conn,\n",
    "                index=False,\n",
    "                if_exists=\"append\"\n",
    "            )\n",
    "            \n",
    "except Exception as e:\n",
    "    print(e, file=stderr)\n",
    "    sqlite_conn.rollback()\n",
    "\n",
    "finally:\n",
    "    sqlite_conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlite_conn = get_sqlite_conn(SCAN_SQLITE)\n",
    "\n",
    "try:\n",
    "    with sqlite_conn:\n",
    "        # GBIF stores it as scientificName, SCAN stores it as sciName\n",
    "        sqlite_conn.execute(\"update omoccurrences set scientificName = sciName where source = 'scan'\")\n",
    "        \n",
    "        # Cleaning\n",
    "        sqlite_conn.execute(\"update omoccurrences set scientificName = trim(scientificName)\")\n",
    "except Exception as e:\n",
    "    print(e, file=stderr)\n",
    "    sqlite_conn.rollback()\n",
    "finally:\n",
    "    sqlite_conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_sciname(row):\n",
    "    rank_col = row[\"taxonRank\"].lower()\n",
    "    if rank_col in row:\n",
    "        row[\"scientificName\"] = row[rank_col]\n",
    "    return row\n",
    "\n",
    "def remove_parenthesis(row):\n",
    "    sciName = row[\"scientificName\"]\n",
    "    \n",
    "    if pd.isna(sciName):\n",
    "        return row\n",
    "    \n",
    "    sciName = re.sub(r\"\\([^)]+\\)\", \"\", sciName)\n",
    "    sciName = re.sub(r\" +\", \" \", sciName).strip()\n",
    "    row[\"scientificName\"] = sciName\n",
    "    return row\n",
    "\n",
    "def remove_authorship(row):\n",
    "    sciName = row[\"scientificName\"]\n",
    "    \n",
    "    if pd.isna(sciName):\n",
    "        return row\n",
    "    \n",
    "    sciName = re.sub(r\" [A-Z][a-z]+,\\s+\\d{4}\\s*$\", \"\", sciName)\n",
    "    sciName = re.sub(r\" +\", \" \", sciName).strip()\n",
    "    row[\"scientificName\"] = sciName\n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== FILTERING =====\n",
    "\n",
    "CHUNK_SIZE = 100000\n",
    "N=10\n",
    "\n",
    "sqlite_conn = get_sqlite_conn(SCAN_SQLITE)\n",
    "    \n",
    "try:\n",
    "    with sqlite_conn:\n",
    "        db_df = pd.read_sql(\"select occid, scientificName from omoccurrences\", sqlite_conn, chunksize=CHUNK_SIZE)\n",
    "        for chunk in db_df:\n",
    "#             print(\"Original\")\n",
    "#             display(chunk.head(n=N))\n",
    "            \n",
    "            chunk = chunk.apply(remove_parenthesis, axis=\"columns\")\n",
    "            \n",
    "#             print(\"Without parentheses\")\n",
    "#             display(chunk.head(n=N))\n",
    "            \n",
    "            chunk = chunk.apply(remove_authorship, axis=\"columns\")\n",
    "            \n",
    "#             print(\"Without authorship\")\n",
    "#             display(chunk.head(n=N))\n",
    "            \n",
    "            chunk_valid_scinames = chunk[\n",
    "                ~(pd.isna(chunk[\"scientificName\"]) | (chunk[\"scientificName\"] == ''))\n",
    "            ]\n",
    "            for _, row in chunk_valid_scinames.iterrows():\n",
    "                sqlite_conn.execute(\n",
    "                    \"UPDATE omoccurrences SET scientificName = ? WHERE occid = ?\",\n",
    "                    (row[\"scientificName\"], row[\"occid\"])\n",
    "                )\n",
    "\n",
    "except Exception as e:\n",
    "    sqlite_conn.rollback()\n",
    "    sqlite_conn.close()\n",
    "    raise e\n",
    "\n",
    "sqlite_conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
